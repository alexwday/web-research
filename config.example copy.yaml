# Deep Research Agent Configuration
# Copy this to config.yaml and customize

# =============================================================================
# LLM PROVIDER SETTINGS
# =============================================================================
llm:
  # Provider: "anthropic", "openai", "openrouter"
  provider: "anthropic"
  
  # Model selection by task type
  models:
    planner: "claude-sonnet-4-20250514"      # High intelligence for planning
    researcher: "claude-sonnet-4-20250514"   # Fast for research loops
    writer: "claude-sonnet-4-20250514"       # High quality for final writing
    editor: "claude-sonnet-4-20250514"       # For final compilation
  
  # Token limits
  max_tokens:
    planner: 8000
    researcher: 4000
    writer: 6000
    editor: 8000
  
  # Temperature settings (0.0 = deterministic, 1.0 = creative)
  temperature:
    planner: 0.3
    researcher: 0.2
    writer: 0.4
    editor: 0.2

# =============================================================================
# SEARCH & SCRAPING
# =============================================================================
search:
  # Primary search provider: "tavily", "serper", "brave"
  provider: "tavily"
  
  # Search depth: "basic" or "advanced"
  depth: "advanced"
  
  # Max results per search
  max_results: 8
  
  # Search queries per task
  queries_per_task: 3
  
  # Include domains (optional, leave empty for all)
  include_domains: []
  
  # Exclude domains
  exclude_domains:
    - "pinterest.com"
    - "quora.com"

scraping:
  # Max content length per page (characters)
  max_content_length: 15000
  
  # Request timeout (seconds)
  timeout: 15
  
  # User agent rotation
  rotate_user_agents: true
  
  # Respect robots.txt
  respect_robots: true
  
  # Retry attempts for failed scrapes
  max_retries: 3

# =============================================================================
# RESEARCH PARAMETERS
# =============================================================================
research:
  # Minimum tasks to generate in initial plan
  min_initial_tasks: 10
  
  # Maximum tasks (prevents infinite recursion)
  max_total_tasks: 200
  
  # Maximum recursion depth for sub-topics
  max_recursion_depth: 3
  
  # Minimum word count per section
  min_words_per_section: 500
  
  # Maximum word count per section
  max_words_per_section: 3000
  
  # Required citations per section
  min_citations_per_section: 2
  
  # Enable recursive task discovery
  enable_recursion: true
  
  # Pause between research sessions (seconds)
  session_delay: 2
  
  # Maximum runtime (hours) - 0 for unlimited
  max_runtime_hours: 24
  
  # Maximum loops - 0 for unlimited
  max_loops: 0

# =============================================================================
# OUTPUT SETTINGS
# =============================================================================
output:
  # Output directory
  directory: "report"
  
  # Final report filename (without extension)
  report_name: "DEEP_RESEARCH_REPORT"
  
  # Export formats: ["markdown", "html", "pdf"]
  formats:
    - "markdown"
    - "html"
  
  # Include table of contents
  include_toc: true
  
  # Include bibliography
  include_bibliography: true
  
  # Include glossary
  include_glossary: true
  
  # Include executive summary
  include_summary: true

# =============================================================================
# QUALITY CONTROL
# =============================================================================
quality:
  # Validate citations exist
  validate_citations: true
  
  # Check for duplicate content
  check_duplicates: true
  
  # Minimum source quality score (0-1)
  min_source_quality: 0.5
  
  # Require academic sources
  prefer_academic: false
  
  # Fact-check critical claims
  fact_check: false

# =============================================================================
# RATE LIMITING
# =============================================================================
rate_limits:
  # LLM API calls per minute
  llm_calls_per_minute: 20
  
  # Search API calls per minute
  search_calls_per_minute: 10
  
  # Web scraping requests per minute
  scrape_requests_per_minute: 30

# =============================================================================
# LOGGING & MONITORING
# =============================================================================
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR
  level: "INFO"
  
  # Log to file
  file: "logs/research.log"
  
  # Log rotation (MB)
  max_file_size: 10
  
  # Keep N backup files
  backup_count: 5
  
  # Show progress in console
  show_progress: true

# =============================================================================
# DATABASE
# =============================================================================
database:
  # SQLite database file
  path: "research_state.db"
  
  # Enable WAL mode for better concurrency
  wal_mode: true
